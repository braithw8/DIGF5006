# DIGF5006 - New Interfaces for Musical Expression - OCAD University
## Blog
### Post 1 - May 18, 2018
#### Performer Audience Contract
A most interesting tangent emerged during our first 'sounding' session as part of Adam Tindale's new course New Interfaces for Musical Expression at OCAD University. If that's familiar tituallary, it's an homage to the course's inspiration, NIME – The International Conference on New Interfaces for Musical Expression. It's the first iteration of the course, but Adam's connection to the conference and his depth of experience forecasts an exciting, engaging, and experimental six weeks.

Back to the tangent, in our first experimental listening and performance 'sounding' session, we embarked on a lively debate on the definition of a performance in a musical context. More specifically, we explored the figurative contract a performer has with their audience.

Are there requirements of a performer that qualify them as a performer and the event a performance? If so, what are they?

Does a musician need to perform using their voice or other musical instrument in a performance? If yes, further gradations arise in the variances and possibilities of musical instruments themselves. Electronic instruments are ubiquitous in contemporary performance, but there's a wide range of interactions and engagement with the the apparatus that divides opinion on the validity of the resulting performance. Adam had a great example of the Orb, pushing play at the top of their set and then engaging in a game of chess for the duration of the musical experience. The interesting thing about this example is that no effort was spent trying to deceive the audience as to their on-stage activities or their real-time influence over the music playback. There's an honesty to this that I appreciate and perhaps this is the core of the contract; connect with your audience without embellishing the fundamental nature of your performance. If you’re singing, sing. If you’re turning a knob and grimacing, that knob should be doing something. This is not to say there should be no embellishment as that would make for mundane experiences. However, to suggest a sonic contribution where there is none only serves to create friction between the audience and performer if the deception is revealed.

There are examples where this friction is embraced and exploited as an element of the performance. The KLF was one such act that had a complex and sometimes antagonistic relationship with their audience. One of my favourite clips is their performance as the Timelords on Tops of the Pops, performing their early hit ‘Doctorin’ the Tardis’. The robust performance and mimicry create strong moments of synchresis but it’s obvious to any fan of popular music that the only music other than vocals are samples of the Doctor Who theme and Gary Glitter’s Rock and Roll Part 2. The deceit is playfully revealed when the drummers cease drumming for dramatic effect, but the recorded drum track keeps chugging along.

<iframe width="560" height="315" src="https://www.youtube.com/embed/zKQhB9Z5Jxg?start=49" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>



### Post 2 - May 31, 2018

#### Code as performance

As part of the NIME course, each student is reading and presenting on papers from the NIME archive. Each week, we each examine both a self-selected and randomly chosen paper from a range of years. This past week looked at  2001 through 2005, the founding years of the conference. It was daunting to source a single entry from the wealth of papers available. Most of the topics warranted further investigation of the abstract. I skimmed through half a dozen papers before finally settling on Ge Wang and Perry Cook’s *On-the-fly programming: using code as an expressive musical instrument*.

What interested me in this paper was the idea of live-programming as a musical performance. I had heard about live coding jams where a coder would start a new program from scratch in a performance setting, but I had never considered a musical performance created by code in realtime. Perhaps this is the beauty of continuing education: small but important revelations on what’s possible in different contexts. As an electronic performer myself, I’m well versed in the use of electronic and digital instruments in performance but live code had eluded my gaze. Using digital instruments, one is working with code, but at a higher-order surface-level connection with pre-coded control over specific parameters.

As the paper spells out, a live coding performance uses text editor and a live compiler. It’s the latter that really makes this possible, with the authors creating Chuck, a bespoke compiler for live musical coding. The main challenges of their work was creating a modular system that allowed for elements to be added and removed from a live running program. Secondly, this timing of this system must be robust enough to support a musical framework and not lose the beat or hiccup while modifying the existing program. The following is a key passage that illustrates the moment of submitting new code to the compiler for live inclusion into the running program. This is the magic of Chuck

> Add – type-checks and compiles a new shred (from a ChucK source file, a string containing ChucK code, or a pre-compiled shred). If there are no compilation errors, the shred is allocated and sporked in the virtual machine with an unique ID. A new virtual stack is allocated, and the shred is shreduled immediately to execute from the beginning. When add fails due to compilation errors, the virtual machine continues to run as before while the

I really appreciate the colourful language such as *sporked* and *shreduled* that characterizes the Chuck programming language.

The first video illustrates a Chuck performance from the computer performance. The latter illustrates a Chuck performance from the audience perspective.

<iframe width="560" height="315" src="https://www.youtube.com/embed/D_JSDNoc4Gs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/ZXuZpAYqmco" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

#### scoreTable

My randomly selected paper, *Mary had a little scoretable* or the reactable* goes melodic* complemented Ge Wang’s paper nicely. This paper by Sergi Jordà and Marcos Alonso introduces a sidecar composition and arrangement interface for their previous work, the reacTable. Backing up for a second, the reacTable is a mindblowing tabletop interface that optically tracks objects (fiducials) on a table and uses their presence and orientation on the table as inputs into the reacTable musical interface. Furthermore, a GUI is projected onto the surface of the table, providing visual feedback of the interaction.

<iframe width="560" height="315" src="https://www.youtube.com/embed/Mgy1S8qymx0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

The interesting takeaway from this article is the comparison of the reacTable to traditional musical instruments. While the reacTable is a fantastic platform to explore timbre, arrangement, and other musical parameters, its ability to ‘musically’ pitch its performance is limited. Referencing Perry Cook’s comments at the inaugural NIME that a instrument should be able to perform *Mary Had a Little Lamb*, the team created the scoreTable, in part, to satisfy that challenge. It raises an interesting debate on the validity of a musical instrument, primarily on the basis of pitch. As the reacTable team argued, their device could do so much more than other instruments and pitch was not of primary concern.

I would argue that pitch is only one parameter of many that define a musical instrument and that any instrument can be placed on a spectrum between pure tones and broadband noise. In our lively in-class debate we explored instruments that are as close to pure tones or sine waves as possible, shakuhachi flutes as an example. I received some flak from our percussionist-heavy group by suggesting that drums exist on the other end of the spectrum. I was treated to a drums only version of *When the Saints Go Marching In* as a rebuttal. I think my argument was taken as anti-drum, whereas my intention was to be inclusive of all instruments on the pitch-ability spectrum as musical instruments.

I for one am fascinated by one-note songs where standard melodic instruments do not deviate from a single pitch. While not-suitable-for-work (if you speak French), the following is one of the best examples.

<iframe width="560" height="315" src="https://www.youtube.com/embed/NgSWm9_BZjs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

### Post 3 - June 22, 2018

Fascinating conversations and in-class explorations are only one component of OCAD's DIGF5006, *New Interfaces for Musical Expression*. As our major project we're developing our own musical interfaces while authoring a supporting paper in the NIME format. In my short time in the *Digital Futures* masters program at OCAD, I've already explored the creation of three musical interfaces and I'm eager to continue this exploration. Here's a quick summary of my musical work to date in my master's studies.

#### Previous Work
##### Harmonizer

The development of the Harmonizer was inspired by a fascination with the differences between *just* and *equal* intonation of musical instruments. The Harmonizer is an audible frequency interval reference that can switch between just and equal intonation. Essentially, one can build chords and then compare the difference between tuning systems. Coded in JavaScript, the Harmonizer featured a physical interface of eight potentiometers connected to the browser via a microcontroller attached via a serial connection. The current iteration uses P5.js' sound add-on library. My next iteration will feature a web interface and will be rebuilt using flocking.js.

[http://blog.ocad.ca/wordpress/digf6037-fw201702-01/2017/11/harmonizer/](http://blog.ocad.ca/wordpress/digf6037-fw201702-01/2017/11/harmonizer/)

![Harmonizer](images/harmonizer.png)

##### Danger zone

This was a fun and flippant game I created in Unity. You are driving a car on the inside of a cube. Six buttons determines the vector of gravity, determining the surface that you drive one. In addition, each side of the cube portrays time differently with the music playing back at a different rate or direction depending on the side of the cube.

[https://braithwaite-finlay.format.com/blog/digital-games-blog-003-danger-zone](https://braithwaite-finlay.format.com/blog/digital-games-blog-003-danger-zone)

![Danger Zone](images/dangerzone.png)

##### Junior Jazz Hands

I lost myself in developing this music improvisational gaming tool. Through engaging a musical framework in play, players learn to play in key, form chords, and improvise. Future iterations will feature two-player modes where game theory models of Nash equilibrium will reward players for improvising successfully with one another.

[https://webspace.ocad.ca/~3164558/JJH/](https://webspace.ocad.ca/~3164558/JJH/)

![Junior Jazz Hands](images/jjh.png)

#### Introducing V-aural

[V-auRal .git](https://github.com/braithw8/V-auRal)

Taking this work of developing musical interfaces further while exploring technologies currently unfamiliar to me, I resolved to create V-Aural, a virtual reality navigational synth. In short, through navigating space in virtual reality you are controlling the parameters of a synth.

Various parameters of navigation are mapped to parameters of the synthesizer engine. For example, the player's current position XYZ position in 3d space is mapped to the pitch of three oscillators. The player's velocity is mapped to the intensity of frequency modulation and rotation is mapped to amplitude modulation.

Through this exploration I am connecting the work I did in JavaScript and Unity. Unity is the front-end of the system, connecting with the VR input; in my case an HTC Vive. The backend is handled by my first implementation of flocking.js through node.js. Also new to me is OSC which forms the conduit between Unity and node.js.

![V-auRal Flow](images/V-auRal.svg)

Developing in VR is a pain in the neck, with constantly the constant donning and removal of the headset, so I've begun my work with a screen and an XBOX one controller as my input. So far, I've successfully connected my 3d object in Unity to flocking.js. There's still work to do in refining the 3d object itself as well as its mapping to flocking. For now though, there's a solid framework to continue developing.

The largest remaining challenge is to connect flocking.js' output back to the VR environment. My desire is to seamlessly integrate the object's audio into the user's aural three-dimensional perspective. So, if the object is above you, it sounds like it's above you. And if your position or orientation changing, your perspective on the object changes to match.

This is a work in progress, but check this space for the conclusion of my work on V-auRal.

<iframe width="1072" height="1351" src="https://www.youtube.com/embed/L8C2jTxxn8M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

### Post 3 - July 8, 2018

The first working prototype of V-auRal is complete. While there are many elements to finesse and directions not explored in the first iteration, I am nonetheless excited by it's potential as an engaging addition to the world of musical interfaces. I'd argue it adds a unique exploration of sonification through positional navigation as its contribution to the field of new musical interfaces.

## V-auRal Experience

The following is an excerpt of the supporting paper for V-auRal written in the style of a NIME conference paper. It's a succinct documentation of the V-auRal user experience.

[Full Paper](https://drive.google.com/file/d/1i80l4zSsFRtVw4y9VM1oMcsh7cteWWJx/view?usp=sharing)

> In the current V-auRal iteration, the performer pilots a three-dimensional object using a standard gaming controller with a joystick and three buttons. The object is external to the performer's fixed position. The rationale for this design choice is to allow for spatial positioning of the synthesized audio as a core element of the experience. The player rotates the object using the joystick and can modify the axis of rotation with one of the buttons. The remaining two buttons activate positive or negative thrust on the game object, propelling it through space. To aid with visual navigation, the object is arrow-shaped.

> The performer's position, velocity, and rotation map to parameters of a three-oscillator synthesizer. The performer shapes their performance with their navigation, creating an ever-morphing tonal mélange.

> The performer's movement in space is sonified and ultimately delivered to the performer's headphones. This audio foldback is has a spatial perspective that matches the performer's perspective. The performer can sense the relative position of the object through this 3D aural perspective. Additionally, the performer is able to sense movement of the object passing by, as a noticeable doppler effect is audible.

This explanation of the experience's details fails to convey how awesome it is to play V-auRal and how expressive and cool it sounds. Navigating your avatar in 3D space creates a dynamically textured design that's out of this world. Beyond that, it's really fun to fly. The spatial audio features evokes a palpable visceral response as the avatar flies by the user's perspective. The spatial engine truly makes the experience immersive with the aural position directly matching the visual; all the while responding to the user's changing perspective. I could hype this all I want but text is failing me in expressing the engagement experienced by the user. It sounds corny but you have to hear it to believe it.

The following is a demonstration video of V-Aural. Self-documentation of VR is incredibly difficult. Thanks to Tommy Ting (Digital Futures) for filming this. It is also difficult to screen capture a program that spans multiple development environments. Current Game DVR software connects directly to the GPU output for a single application. I used ActivePresenter to capture the development environment in it's entirety in glorious 4k.

<iframe width="560" height="315" src="https://www.youtube.com/embed/1cuc1pm2cJo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

## Development Notes

This project checked a lot of boxes in terms of technologies I wanted to explore but hadn't yet had the chance. Through this exploration, I now have a taste for VR integration with the Unity Gaming engine. Using the Vive, I found it relatively straightforward to connect the VR headset to my existing Unity project. This is a testament to the Steam VR engine, its Unity plug-in, and the documentation that comes with it. There's still much to investigate; my biggest disappointment was relying on a traditional game controller as my input rather than integrating the Vive controller in my work.

Similarly, I scratched the surface of OSC in developing V-auRal. I've been using MIDI for over twenty years, it was time for an update! With my relatively modest integration of OSC to make direct mappings between Unity and Flocking.js, I'm a believer in the power of OSC. Perhaps it's the ability to openly code and process OSC data in a scripting language such as JavaScript that most impresses me. The combination of the two point to endless possibilities of musical mappings and interactions. A special shout out goes to UniOSC, the well packaged commercial OSC library for Unity. While there were many free libraries available, there wasn't the time in this six week crash course to interpret and debug cheap and cheerful libraries. While the documentation could have been more polished, I found the graphical OSC editor and data console to be invaluable in controlling and interpreting the OSC data I was creating.

Perhaps the oddest fit in the whole workflow, I thoroughly enjoyed the introduction to Node.js that this project facilitated; its necessity borne out the inability of the Chrome browser to accept UDP based OSC. Had I not chosen Flocking as my synthesis engine, PD for example, I wouldn't not have glimpsed into the massive potential of Node.js. Who thought JavaScript without the browser could be such a powerful tool for creating powerful cross-platform applications. Now, don't get me wrong - I wrestled with node and NPM to the point that I almost gave up all hope. However, through the guidance of Adam Tindale, I persevered in creating my first functional Node.js application. There's a long way to go in terms of finessing and developing my work in this environment as, for example, I'd love to learn the ways of Electron to package an application that appears more polished than what I've made here.

One of my biggest regrets is not delving into the potential of Flocking.js further. With so many working parts, skimming the surface of technologies is a common theme in this development. Flocking appears to be an amazing synthesis engine, but it will take more time and other projects to get a better sense of how I can integrate it into my future work. While this excites me, it is also somewhat unclear as to the shape this future exploration will take.

The biggest surprise of my development was the exploration of positional audio in Unity. I knew this was all possible, but I was not prepared for the aural excitement and personal engagement it created. When I heard the doppler effect of a game object I created as it whizzed by, I was transfixed. I moved that object around for a good 30 minutes and forced a good portion of my cohort to share the experience. I have since offered my fledgling but excited experience with spatial sound in Unity to others in my cohort, in hopes that I can make a small but meaningful contribution to their thesis projects.

While excited about spatial audio, I was less thrilled with the awkward mechanics of shuttling audio back from Node.js into Unity. Using a loopback audio driver created noticeable latency as well as some pop and click artifacts inherent in the connection. However, the spatial engine saved the day as it added a layer of immediate feedback back into the project. The synthesizer was quite latent, but it's navigation in space wasn't.

In closing, I'm proud of the work I've started here. I'm eager to continue with the project but I've now found myself in the middle of a whirlwind summer thesis curriculum. I'm amazed what can be accomplished in relatively short amount of time, but I appreciate that much more time is required to take V-auRal to the next level.
